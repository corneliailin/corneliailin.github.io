{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a Spark Cluster \n",
    "### (with Hadoop/HDFS)\n",
    "\n",
    "\n",
    "Author: Cornelia Ilin <br>\n",
    "Email: cilin@ischool.berkeley.edu <br>\n",
    "Date: July 26, 2020\n",
    "\n",
    "`Notebook status:` in progress\n",
    "\n",
    "`Audience:` Written with Python users in mind. I will ocasionally add some notes for R users.\n",
    "\n",
    "\n",
    "`Citations:` I recommend reviewing all of them (especially source 4)\n",
    "\n",
    "1. CloudLab: https://www.cloudlab.us\n",
    "\n",
    "2. Hadoop cluster setup: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/ClusterSetup.html\n",
    "\n",
    "3. Spark cluster setup: http://spark.apache.org/docs/latest/spark-standalone.html\n",
    "\n",
    "4. Awesome book on Hadoop/HDFS/Spark: http://grut-computing.com/HadoopBook.pdf\n",
    "\n",
    "5. More notes on Hadoop/Spark cluster setup: [link1](http://pages.cs.wisc.edu/~akella/CS838/F15/assignment1.html); [link2](http://pages.cs.wisc.edu/~akella/CS838/F15/assignment2.html)\n",
    "   \n",
    "6. Secure file transfer: https://linuxize.com/post/how-to-use-scp-command-to-securely-transfer-files/\n",
    "\n",
    "7. PySpark for Jupyter notebooks: [link1](https://opensource.com/article/18/11/pyspark-jupyter-notebook); [link2](https://mortada.net/3-easy-steps-to-set-up-pyspark.html)\n",
    "\n",
    "8. Hadoop commands reference: https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/CommandsManual.html\n",
    "\n",
    "9. Bash (terminal) commands: https://devhints.io/bash\n",
    "\n",
    "\n",
    "``For Python users``\n",
    "\n",
    "[ADD Here]\n",
    "\n",
    "\n",
    "``For R users``\n",
    "\n",
    "How to add an R kernel: conda install -c r r-irkernel\n",
    "\n",
    "How to add packages: https://docs.anaconda.com/anaconda/packages/r-language-pkg-docs/#t-packages\n",
    "\n",
    "How to fix errors: R libreadline error: [link1](https://askubuntu.com/questions/1168787/libreadline-so-6-issue-in-ubuntu-18-04); [link2](https://stackoverflow.com/questions/23993377/red-language-console-error-libreadline-so-6-cannot-open-shared-object-file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start by creating an experiment and topology in CloudLab (Note: you need to create an account).\n",
    "\n",
    "https://www.cloudlab.us"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 1: Create experiment and topology\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Check Resource Availability on CloudLab``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiments -> Resource Availability (copy the ID of a CloudLab server you want to request)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Reserve nodes and create topology``\n",
    "\n",
    "Experiments -> Create Experiment Profile -> [ADD Name] -> Create Topology -> [DRAG Bare Metal PC (e.g., x2 if 2 nodes)] -> [LINK nodes] -> [CLICK on each node (select hardware type] -> Accept -> Create -> Instantiate -> Next -> Finish.\n",
    "\n",
    "Experiments -> My Experiments -> [CLICK your experiment name] -> List View -> [COPY IP addresses for each node (you will use these to login remotely to each node].\n",
    "\n",
    "To extend experiment: Experiments -> My Experiments -> [CLICK your experiment name] -> List View -> [COPY IP addresses for each node] -> Extend."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Short description of our Experiment``\n",
    "\n",
    "I reserved 4 nodes, each with about 220GB of RAM. The topology I created in CloudLab shows that these nodes are named 'node-0', 'node-1', 'node-2', node-3'.\n",
    "\n",
    "We will define 'node-0' as our Master node. And we will define 'node-0', 'node-1', 'node-2', node-3' as our Worker nodes. Note that 'node-0' is both a Master and a Worker node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 2: Generate a SSH key on your local machine\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Definitions:``\n",
    " - Local machine == the machine you will use to connect to the cluster of nodes.\n",
    "\n",
    " - A SSH key is used for automating logins, single sign-on, and for authenticating hosts.\n",
    "\n",
    "``Comments:``\n",
    "\n",
    "When you run the *ssh-keygen -t rsa* command:\n",
    "  - Enter file in which to save the key: [if you leave it blank it will be saved in the id_rsa.pub file].\n",
    "  - Enter passphrase: [leave it empty].\n",
    "  - Enter same passphrase again: [leave it empty].\n",
    "\n",
    "When you run *less id_rsa.pub* command:\n",
    "  - copy the key and add it to CloudLab ([SELECT your user name] -> Manage SSH Keys.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh-keygen -t rsa\n",
    "!ls -la\n",
    "!less id_rsa.pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 3: Connect to each node from the command line\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will open a Shell tab for each node and type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh cilin@c220g5-111028.wisc.cloudlab.us #change this with the IP address of each node (see Step1 for location of IP address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 4: Establish passwordless SSH connection between nodes\n",
    "---    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, check to see if you can connect from one node to another.\n",
    "\n",
    "Run the code below in the Shell corresponding to the main node (node-0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh node-1 #note: this command is run on node-0. It tries to connect to node-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If permission is denied (very likely), create a public key on the main node (node-0) and propagate the key to the other nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh-keygen -t rsa -f node1 #note: this command is run on node-0\n",
    "!ls -la\n",
    "!less id_rsa.pub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To propagate the node-0 key to the other nodes, add it to CloudLab ([SELECT your user name] -> Manage SSH Keys. See Step 2 comments related to the output of the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 5: Mount more hard drive on each node\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At instantiation, the hard drive space allocated (mounted) for use on each Linux node is usually very small. So we need to change this because our data needs to sit there.\n",
    "\n",
    "Definitions:\n",
    " - Hard drive naming convention in Linux: what you need to know is that there is no C drive or E drive in Linux. You will see something like /dev/sda, /dev/sdb, /dev/sdc, â€¦ etc. instead (these are called hard disk paritions in Linux). The dev is short for device. The sd was short for Small Computer System Interface (SCSI) mass-storage driver. Below is an example figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='/Users/cilin/Research/hprm/cloud_setup/images/Linux_hard_drive.png', width=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check how much disk space is mounted by default: this information is available under */dev/sda, /dev/sdb, /dev/sdc*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!df -h #command for a single node\n",
    "!for i in {5:8}: do ssh node-${i} \"df -h\" #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now check what other partitions are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!lsblk ##command for a single node\n",
    "!for i in {5:8}: do ssh node-${i} \"lsblk\" #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that we have >1 TB of (unmounted) disk space in /dev/sdb, so let's mount this space on a directory called *storage*. We will do this for each node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 5.1.`` Format dev/sdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkfs.ext4/dev/sdb #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"sudo mkfs.ext4/dev/sdb\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 5.2.`` Create the directory where you will mount this disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir storage # command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir /users/cilin/storage\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 5.3.`` Mount dev/sdb to users/cilin/storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mount -t ext4 /dev/sdb /users/cilin/storage #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"sudo mount -t ext4 /dev/sdb /users/cilin/storage\"; done #command to run on all nodes\n",
    "\n",
    "!df -h #to make sure it worked (runs on a single node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 5.4.`` Change the permissions to users/cilin/storage to be accessed by anyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod -R 777 /users/cilin/storage #command for a single nodes\n",
    "!for i in {5..8}; do ssh node-${i} \"sudo chmod -R 777 /users/cilin/storage\" #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Step 6: Install software on each node\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to install Java, Hadoop, and Anaconda (contains both Python and R) on each node. To keep organized, let's create a directory called *software* to add all programs but Java."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /users/cilin/software #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir /users/cilin/software\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Install Java</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.1.-Java`` Check to see what Java versions are available in Ubuntu's repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-cache search openjdk  #you can check this command only on one node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that two Java versions are available: jdk8 and jdk11. Let's install the latest version, jdk11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.2.-Java`` Install jdk11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to refresh all the repositories in each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"sudo apt-get update\"; done #command to run on all node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, install jdk11 on each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install openjdk-11-jdk #command for a single machine\n",
    "!for i in {5..8}; do ssh node-${i} \"sudo apt-get install openjdk-11-jdk\"; done #command to run on all machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Install Hadoop</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to install the latest version of Hadoop (3.3.0 at the time of writing this notebook) from here: https://hadoop.apache.org/releases.html\n",
    "\n",
    "Copy the link of the binary file: https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz\n",
    "\n",
    "Then run the following in the command line (remember to save Hadoop in the *software* folder):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.1.-Hadoop`` Download the binary .tar.gz file from their website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"cd~/software; wget https://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz; ls\"; done #command to run on all nodes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.2.-Hadoop`` Unzip the binary .tar.gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/sfotware\n",
    "!tar -xzvf hadoop-3.3.0.tar.gz #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"cd~/software; tar -xzvf hadoop-3.3.0.tar.gz; ls\"; done #command to run on all nodes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.3.-Hadoop`` Configure HDFS on each node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HDFS is the equivalent of the file system in your local machine, the only difference is that it runs on multiple machines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> configure core-site.xml and hdfs-site.xml files</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On your local machine, configure the files **core-site.xml** and **hdfs-site.xml** (you can find them in hdfs_config_files). Note that you can also transfer these files to the Master node-5, edit with vim, and then transfer the edited files to all the other nodes. These files will be copied in a directory called *config*.\n",
    "\n",
    "I also provide the core-site[original].xml and hdfs-site[original].xml files for a better understanding on where your edits(configurations) should go.\n",
    "\n",
    "Useful vim commands (if you want to edit in the node):\n",
    " - **i** (stands for insert so you can edit)\n",
    " - **esc** and then **:wq!** to save and exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Create *config* directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /users/cilin/config #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir /users/cilin/config\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) Assuming you have configured the **core-site.xml** and **hdfs-site.xml** files according with the instructions presented there, let's copy them on each node from our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy on Master (node-5) from Local Machine\n",
    "!scp -r ~/Research/hprm/CloudLab/HDFS_config_files* cilin@128.105.144.198:/users/cilin/config \n",
    "\n",
    "# copy on all the other nodes from Master\n",
    "!for i in {6:8}; do scp -r* node-${i}:users/cilin/config; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) Finally, we need to create the directories listed in the **core-site.xml** and **hdfs-site.xml** files on each node.\n",
    "\n",
    "These directories are:\n",
    " - /users/cilin/storage/data/local/tmp\n",
    " - /users/cilin/storage/hdfs/hdfs_nn_dir\n",
    " - /users/cilin/storage/hdfs/hdfs_dn_dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/data/local/tmp #command for a single node\n",
    "for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/data/local/tmp\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/hdfs/hdfs_nn_dir #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/hdfs/hdfs_nn_dir\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/hdfs/hdfs_dn_dirs #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/hdfs/hdfs_dn_dirs\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> configure run.sh file for Hadoop</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will follow the same steps as above to ocnfigure the *run.sh* file (you can find it in run_files). We will copy the run.sh file on the *home directory* of each machine.\n",
    "\n",
    "You can see that the *run.ch* files contains configurations for both Hadoop and Spark. I recommend you edit the configurations for Spark too because we will need them later on.\n",
    "\n",
    "I also provide the run[original].sh file for a better understanding on where your edits(configurations) should go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) Assuming you have configured the **run.sh** file according with the instructions presented there, let's copy it on each node from our local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy on Master (node-0) from Local Machine\n",
    "!scp -r ~/Research/hprm/CloudLab/Run_files/run.sh cilin@128.105.144.198:/users/cilin/ #the IP address here is Master node-0's IP.\n",
    "\n",
    "# copy on all the other nodes from Master\n",
    "!for i in {6:8}; do scp -r run.sh node-${i}:users/cilin/; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) We need to create the Hadoop related directories listed in the **run.sh** file on each node.\n",
    "\n",
    "These directories are:\n",
    " - /users/cilin/storage/logs/hadoop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/logs/hadoop #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/logs/hadoop\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) Create a file called *instances* that lists the IP eddress (on a separate line) of each node.  This file should be created in the home directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!vim instances #after this you edit the doc as indicated above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(D) In the run.sh file, you can see a command *pdsh* (PDSH is a very smart little tool that enables you to issue the same command on the nodes specified in the file *instances*). So we need to install *pdsh* on every node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get install pdsh #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"sudo apt-get install pdsh\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(E) We need to make the functions defined in run.ch available in the terminal of each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/run.sh #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"source ~/run.sh\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(F) Finally, we need to add the run.ch file in the .bashrc file and then source the .bshrc file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add run.ch to .bashrc\n",
    "!vim .bashrc #this command has to be run on each node; commands once in edit mode: i, make edits (source ~/run.sh), esc, :wq!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source .bshrc (or connect again to all nodes)\n",
    "!source ~/.bshrc #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"source ~/.bashrc\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> format the HDFS system file</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has to be done only on the Master (node-0). In HDFS terminology, the Master node is called the NameNode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop namenode -format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> connect from Master node to Worker nodes</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will connect using the IPs from the *instances* file. The idea here is to allow HDFS to connect automatically to each worker node without confirmation. This confirmation is required only once, so we want to do it in this setup stage. \n",
    "\n",
    "Important: don't forget to connect from Master node to Master node as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ssh 128.105.144.182 #this connects to node-1 (the IP address here belongs to node-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> start HDFS </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start_hdfs #this function is available in the run.ch file; run it only on the Master node-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> check that DataNode and NameNode exist</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: DataNode and NameNode ==  Master node and Worker node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps #check this on each node (note that Worker nodes only have a NameNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> access HDFS using GUI</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The http:// address is the IP of the Master node-0 followed by the port 50070 (this is the port we defined in hdfs_site.xml file).\n",
    "\n",
    "http://128.105.144.198:50070/dfshealth.html#tab-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> copy files to HDFS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) first, copy the data from Local Machine to the Master (node-0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In *storage*, let's create a folder named *data_to_copy_in_hdfs*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this command is run from Master (Node-0)\n",
    "!mkdir ~/storage/data_to_copy_in_hdfs\n",
    "\n",
    "# this command is run form local machine\n",
    "!scp -r ~/Research/hprm/data/infogroup/* cilin@128.105.144.198:/users/cilin/storage/data_to_copy_in_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(B) second, copy the data from the Master (node-0) to HDFS.\n",
    "\n",
    "Let's create a folder named 'hdfs_data' in Hadoop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these commands are run from Master (Node-0)\n",
    "!hadoop dfs -mkdir /hdfs_data #/ indicates the home directory \n",
    "\n",
    "!hadoop fs  -copyFromLocal /users/cilin/storage/data_to_copy_in_hdfs/ /hdfs_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(C) check to see if the files are in HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -ls /hdfs_data/data_to_copy_in_hdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Install Spark</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to install the latest version of Spark(3.0.0 at the time of writing this notebook) from here: https://spark.apache.org/downloads.html\n",
    "\n",
    "Copy the link of the binary file: https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
    "\n",
    "Then run the following in the command line (remember to save Spark in the *software* folder):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.1.-Spark`` Download the binary .tgz file from their website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"cd~/software; wget https://www.apache.org/dyn/closer.lua/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz; ls\"; done #command to run on all nodes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.2.-Spark`` Unzip the binary .tgz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ~/sfotware\n",
    "!tar -xzvf hadoop-3.3.0.tar.gz #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"cd~/software; tar -xzvf hadoop-3.3.0.tar.gz; ls\"; done #command to run on all nodes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> configure run.sh file for Spark commands</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(A) We need to create the Spark related directories listed in the **run.sh** file on each node.\n",
    "\n",
    "These directories are:\n",
    " - /users/cilin/storage/data/spark/rdds_shuffle\n",
    " - /users/cilin/storage/logs/spark\n",
    " - /users/cilin/storage/logs/apps_spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/data/spark/rdds_shuffle #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/data/spark/rdds_shuffle\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/logs/spark #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/logs/spark\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p /users/cilin/storage/logs/apps_spark #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"mkdir -p /users/cilin/storage/logs/apps_spark\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> start Spark</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!start_spark #this function is available in the run.ch file; run it only on the Master node-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> check that Master node and Worker node exist</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jps #check this on each node (note that Worker nodes only have a Worker)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:green\"> access Spark using GUI</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The http:// address is the IP of the Master node followed by the port 8080 (this is the port defined on Spark's website).\n",
    "\n",
    "http://128.105.144.198:8080\n",
    "\n",
    "Citation: http://spark.apache.org/docs/latest/spark-standalone.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:red\">Install Anaconda</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to install the latest version of Anaconda from here: https://www.anaconda.com/products/individual#linux\n",
    "\n",
    "Copy the link of the Linux file: https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh\n",
    "\n",
    "Then run the following in the command line (remember to save Anaconda in the *software* folder):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.1.-Anaconda`` Download the linux .sh file from their website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"cd~/software; wget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh; ls\"; done #command to run on all nodes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.2.-Anaconda`` Enter the following to install Anaconda for Python 3.7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bash ~/Software/Anaconda3-2020.02-Linux-x86_64.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.3.-Anaconda`` Source .bashrc for installation to take effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source ~/.bshrc #command for a single node\n",
    "!for i in {5..8}; do ssh node-${i} \"source ~/.bashrc\"; done #command to run on all nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.4.-Anaconda`` Activate base environment or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check step 11 here: https://docs.anaconda.com/anaconda/install/linux/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.4.-Anaconda`` Install PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``Step 6.5.-Anaconda`` Launch Jupyter Notebook/Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On node-0's terminal type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter lab"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
